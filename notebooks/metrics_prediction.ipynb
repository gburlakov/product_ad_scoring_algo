{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe49f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import trained_models_folderpath, model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(df, prefix, new_col_name):\n",
    "    one_hot_cols = [col for col in df.columns if col.startswith(prefix + \"_\")]\n",
    "    \n",
    "    if not one_hot_cols:\n",
    "        raise ValueError(f\"No one-hot columns found with prefix '{prefix}_'\")\n",
    "\n",
    "    extracted = df[one_hot_cols].apply(\n",
    "        lambda row: next((int(col.split(\"_\")[1]) for col in one_hot_cols if row[col]), np.nan), axis=1\n",
    "    )\n",
    "\n",
    "    df[new_col_name] = extracted.astype(\"Int64\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a951bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_metric(trained_models_folderpath, val_last_day_str, X_val, y_val, orig_X_val, target, ad_platform, model_dict):\n",
    "    \"\"\"\n",
    "    Predicts the target values for the validation dataset using trained models.\n",
    "\n",
    "    Args:\n",
    "        trained_models_folderpath (str): path to the folder with trained models\n",
    "        val_last_day_str (str): end date of the validation set\n",
    "        X_val (pd.DataFrame): validation dataset\n",
    "        target (str): target metric to be predicted\n",
    "        target_platform (str): ad platform to predict for, 'meta' or 'google'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ensemble_model_filename = os.path.join(trained_models_folderpath, \\\n",
    "                                             f\"{val_last_day_str}_{target}_ensemble.pkl\")\n",
    "        with open(ensemble_model_filename, 'rb') as f:\n",
    "            ensemble_data = pickle.load(f)\n",
    "        \n",
    "        encoders_filename = os.path.join(trained_models_folderpath, \\\n",
    "                                                         f\"{val_last_day_str}_{target}_encoders.pkl\")        \n",
    "        with open(encoders_filename, 'rb') as f:\n",
    "            encoders = pickle.load(f)\n",
    "\n",
    "        scalers_filename = os.path.join(trained_models_folderpath, \\\n",
    "                                                         f\"{val_last_day_str}_{target}_scalers.pkl\")        \n",
    "        with open(scalers_filename, 'rb') as f:\n",
    "            scalers = pickle.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error loading model or preprocessing objects: {e}.  Make sure training has been run.\")\n",
    "\n",
    "    ensemble_weights = ensemble_data['ensemble_weights']\n",
    "    top_models = ensemble_data['top_models']\n",
    "    features = ensemble_data['features']\n",
    "\n",
    "    X = X_val[features]\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            if col in encoders:  # Check if encoder exists (handle unseen categories)\n",
    "                try:\n",
    "                    X[col] = encoders[col].transform(X[col])\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Unseen category in column {col}, default value (e.g., -1) assigned. Error: {e}\")\n",
    "\n",
    "                    X[col] = X[col].apply(lambda x: encoders[col].transform([x])[0] if x in encoders[col].classes_ else -1)\n",
    "            else:\n",
    "                print(f\"No encoder for column {col}. Encoding skipped.\")\n",
    "                X[col] = 0\n",
    "\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "    for col in numerical_cols:\n",
    "        if col in scalers: #Check if scaler exists\n",
    "            X[col] = scalers[col].transform(X[[col]])\n",
    "        else:\n",
    "            print(f\"Warning: No scaler found for column {col}. Skipping scaling.\")\n",
    "\n",
    "    individual_predictions = {}\n",
    "    for model_name in model_dict.keys():\n",
    "        model_path = os.path.join(trained_models_folderpath, \\\n",
    "                                                 f\"{val_last_day_str}_{target}_{model_name}.pkl\")\n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            individual_predictions[model_name] = model.predict(X).ravel()\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: Model file not found for {model_name}: {e}\")\n",
    "            individual_predictions[model_name] = np.zeros(len(X)).ravel()\n",
    "\n",
    "    ensemble_predictions = np.zeros(len(X))\n",
    "    for model_name, model in top_models.items():\n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            weight = ensemble_weights[model_name]\n",
    "\n",
    "            ensemble_predictions += weight * model.predict(X).ravel()\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: Model file not found for {model_name}: {e}\")\n",
    "            \n",
    "    pred_df = pd.DataFrame(individual_predictions, index=X.index)\n",
    "    pred_df['ensemble'] = ensemble_predictions\n",
    "    pred_df['actual'] = y_val.values\n",
    "\n",
    "    merged_df = orig_X_val.copy()\n",
    "    merged_df[pred_df.columns] = pred_df\n",
    "    \n",
    "    pred_cols = list(individual_predictions.keys()) + ['ensemble']\n",
    "\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    \n",
    "    id_vars = [col for col in merged_df.columns\n",
    "               if col not in pred_cols + ['actual']]\n",
    "\n",
    "    if 'date' in id_vars:\n",
    "        id_vars.remove('date')\n",
    "    id_vars = ['date'] + id_vars\n",
    "    \n",
    "    melted_df = merged_df.melt(id_vars=id_vars, \n",
    "                               value_vars=list(individual_predictions.keys()) + ['ensemble'], \n",
    "                               var_name=\"model\",\n",
    "                               value_name=\"pred_value\")\n",
    "    m = len(pred_cols)\n",
    "    melted_df['actual'] = np.repeat(merged_df['actual'].values, m)\n",
    "\n",
    "    melted_df['target'] = target\n",
    "    melted_df['ad_platform'] = ad_platform\n",
    "    \n",
    "    if 'date' not in melted_df.columns and 'date' in orig_X_val.columns:\n",
    "        melted_df['date'] = orig_X_val['date'].values.repeat(m)\n",
    "\n",
    "    melted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    sort_cols = ['product_id', 'category', 'model', 'date']\n",
    "    for col in sort_cols:\n",
    "        if col not in melted_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in melted_df, skipping in sort.\")\n",
    "    melted_df.sort_values(by=[col for col in sort_cols if col in melted_df.columns], inplace=True)\n",
    "\n",
    "    melted_df.set_index('product_id', inplace=True)\n",
    "    \n",
    "    return melted_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad-score",
   "language": "python",
   "name": "ad-score-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
